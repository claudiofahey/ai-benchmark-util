# Unconstrained configuration - no CPU or memory limits - use for 16 GPUs/sample
slurm_cgroup_config:
  CgroupMountpoint: "/sys/fs/cgroup"
  CgroupAutomount: yes
  CgroupReleaseAgentDir: "/etc/slurm/cgroup"
  ConstrainCores: yes
  TaskAffinity: no
  ConstrainRAMSpace: yes
  ConstrainSwapSpace: no
  ConstrainDevices: no
  AllowedRamSpace: 100
  AllowedSwapSpace: 0
  MaxRAMPercent: 100
  MaxSwapPercent: 100
  MinRAMSpace: 30
slurmdbd_config:
  DbdHost: "nvidia-mgmt"
  StorageType: "accounting_storage/none"
slurm_config:
  AccountingStorageType: "accounting_storage/none"
  ClusterName: "cluster"
  FastSchedule: 1
  GresTypes: "gpu"
  JobAcctGatherType: "jobacct_gather/none"
  MpiDefault: "none"
  ProctrackType: "proctrack/cgroup"
  RebootProgram: "/sbin/reboot"
  ResumeTimeout: 300
  ReturnToService: 2
  SchedulerType: "sched/backfill"
  SelectType: "select/cons_res"
  SelectTypeParameters: "CR_Core"
  ControlMachine: "nvidia-mgmt"
  SwitchType: "switch/none"
  TaskPlugin: "task/affinity,task/cgroup"
  TaskPluginParam: "Sched"
slurm_create_dirs: yes
slurm_create_user: yes
slurm_gres_config:
  - File: "/dev/nvidia[0-15]"
    Name: "gpu"
    NodeName: "dgx2-[1-3]"
    Type: "tesla"
slurm_munge_key: "munge.key"
slurm_nodes:
  - name: "dgx2-[1-3]"
    CoresPerSocket: 24
    Gres: "gpu:tesla:16"
#    MemSpecLimit: 16384
#    RealMemory: 1546833
    Sockets: 2
    ThreadsPerCore: 2
slurm_partitions:
  - name: "gpu"
    Default: "YES"
    MaxTime: "UNLIMITED"
    Nodes: "dgx2-[1-3]"
slurm_user:
  comment: "Slurm Workload Manager"
  gid: 888
  group: "slurm"
  home: "/var/lib/slurm"
  name: "slurm"
  shell: "/usr/sbin/nologin"
  uid: 888
